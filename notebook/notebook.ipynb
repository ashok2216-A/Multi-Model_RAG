{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02789cb",
   "metadata": {},
   "source": [
    "# **Install Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a7db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF -q\n",
    "%pip install gmft -q\n",
    "%pip install sentence-transformers mistralai -q\n",
    "%pip install mistralai --upgrade -q\n",
    "%pip install langchain -q\n",
    "%pip install langchain-community -q\n",
    "%pip install langchain_experimental -q\n",
    "%pip install -U langchain-huggingface -q\n",
    "%pip install --upgrade langchain -q\n",
    "%pip install tiktoken -q\n",
    "%pip install ftfy -q\n",
    "%pip install python-pptx python-docx -q\n",
    "%pip install --upgrade pymilvus requests tqdm tf-keras -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e20ae",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac431de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\rag_app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import ftfy\n",
    "import tiktoken\n",
    "import collections\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from gmft.auto import AutoTableDetector, AutoTableFormatter\n",
    "from gmft.pdf_bindings import PyPDFium2Document\n",
    "\n",
    "# Configration \n",
    "detector = AutoTableDetector()\n",
    "formatter = AutoTableFormatter()\n",
    "\n",
    "tesseract_path = os.path.join(os.getcwd(), \".venv\", \"Tesseract-OCR\", \"tesseract.exe\")\n",
    "os.environ[\"TESSERACT_PATH\"] = tesseract_path\n",
    "\n",
    "DPI = 150  # For OCR resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698b906",
   "metadata": {},
   "source": [
    "# **Document Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efecce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_page_searchable(page, min_chars=10):\n",
    "    return len(page.get_text().strip()) >= min_chars\n",
    "\n",
    "def process_ocr(page, dpi=150, language=None):\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    ocr_pdf_bytes = pix.pdfocr_tobytes(language=language)\n",
    "    return fitz.open(\"pdf\", ocr_pdf_bytes)\n",
    "\n",
    "def extract_tables(pdf_source, page_number):\n",
    "    doc = PyPDFium2Document(pdf_source)\n",
    "    tables = []\n",
    "    try:\n",
    "        page = doc[page_number]\n",
    "        for cropped in detector.extract(page):\n",
    "            formatted = formatter.extract(cropped, margin=\"auto\", padding=None)\n",
    "            df = formatted.df()\n",
    "            tables.append((cropped.bbox, df))\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return tables\n",
    "\n",
    "def extract_tables_from_doc(fitz_doc, page_number):\n",
    "    single_page_doc = fitz.open()\n",
    "    single_page_doc.insert_pdf(fitz_doc, from_page=page_number, to_page=page_number)\n",
    "    pdf_bytes = single_page_doc.tobytes()\n",
    "    single_page_doc.close()\n",
    "    return extract_tables(pdf_bytes, 0)\n",
    "\n",
    "def uniquify_columns(cols):\n",
    "    counts = collections.Counter()\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        counts[col] += 1\n",
    "        new_cols.append(col if counts[col] == 1 else f\"{col}_{counts[col] - 1}\")\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd505",
   "metadata": {},
   "source": [
    "# **Chunking and Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32059d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_utf8(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.replace(\"\\x00\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text = text.replace('\\n', '').replace('\\\"', '')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def custom_chunking(text, chunk_size=500, overlap=50):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    chunks, start = [], 0\n",
    "    while start < len(token_ids):\n",
    "        end = min(start + chunk_size, len(token_ids))\n",
    "        chunk_token_ids = token_ids[start:end]\n",
    "        chunks.append(tokenizer.decode(chunk_token_ids))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def chunking_workflow(text, max_tokens=500, overlap=50):\n",
    "    clean_text = process_text_utf8(text)\n",
    "    if not clean_text.strip():\n",
    "        return []\n",
    "    token_count = count_tokens(clean_text)\n",
    "    if token_count <= max_tokens:\n",
    "        return [{\"text\": clean_text, \"metadata\": {}, \"token_count\": token_count}]\n",
    "    chunks = custom_chunking(clean_text, chunk_size=max_tokens, overlap=overlap)\n",
    "    return [\n",
    "        {\"text\": chunk, \"metadata\": {}, \"token_count\": count_tokens(chunk)}\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "def create_elements_with_metadata(chunks, tables, input_file, text_positions=None, table_positions=None):\n",
    "    elements = []\n",
    "    base_name = os.path.basename(input_file)\n",
    "    # Add text chunks with position if available\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "        metadata.update({\n",
    "            \"source_document\": base_name, \"chunk_id\": i, \n",
    "            \"position\": text_positions[i-1] if text_positions and i-1 < len(text_positions) else None})\n",
    "        elements.append({\"type\": \"text\",\"content\": chunk['text'],\"metadata\": metadata,\"token_count\": chunk['token_count']})\n",
    "\n",
    "    # Add tables with position if available\n",
    "    for j, table in enumerate(tables, start=i+1):\n",
    "        if isinstance(table, dict) and \"metadata\" in table:\n",
    "            # PDF table with metadata\n",
    "            meta = table[\"metadata\"].copy()\n",
    "            meta.update({\"source_document\": base_name,\"chunk_id\": j})\n",
    "            content = table.get(\"content\", table)\n",
    "        else:\n",
    "            meta = {\n",
    "                \"source_document\": base_name,\"chunk_id\": j, \n",
    "                \"position\": table_positions[j - (i+1)] if table_positions and (j - (i+1)) < len(table_positions) else j}\n",
    "            content = table\n",
    "        elements.append({\"type\": \"table\",\"content\": content,\"metadata\": meta})\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb577743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_pages(input_pdf):\n",
    "    src = fitz.open(input_pdf)\n",
    "    ocr_doc = fitz.open()\n",
    "    tables, text_blocks = [], []\n",
    "    text_positions, table_positions = [], []\n",
    "    filename = os.path.basename(input_pdf)\n",
    "\n",
    "    for i, page in enumerate(src):\n",
    "        print(f\"   📄 Processing Page {i + 1}/{len(src)}\")\n",
    "        searchable = is_page_searchable(page)\n",
    "        temp_doc = None  # To manage OCR temp doc lifetime\n",
    "        if not searchable:\n",
    "            print(f\"      🔎 Searchable: ❌ No, requires OCR\")\n",
    "            temp_doc = process_ocr(page, dpi=DPI)\n",
    "            blocks_page = temp_doc[0]\n",
    "            page_tables = extract_tables_from_doc(temp_doc, 0)\n",
    "            ocr_doc.insert_pdf(temp_doc)\n",
    "        else:\n",
    "            print(f\"      🔎 Searchable: ✅ Yes\")\n",
    "            blocks_page = page\n",
    "            page_tables = extract_tables_from_doc(src, i)\n",
    "\n",
    "        for idx, (bbox, df) in enumerate(page_tables, 1):\n",
    "            df.columns = uniquify_columns(df.columns.astype(str))\n",
    "            tables.append({\n",
    "                \"type\": \"table\",\n",
    "                \"content\": df.to_dict(orient=\"records\"),\n",
    "                \"metadata\": {\"page_number\": i + 1,\"columns\": df.columns.tolist(),\n",
    "                    \"table_index_on_page\": idx,\"position\": bbox[1], \"source_document\": filename}})\n",
    "            table_positions.append(bbox[1])  # Save y-position\n",
    "\n",
    "        table_boxes = [box for box, _ in page_tables]\n",
    "        for block in blocks_page.get_text(\"blocks\"):\n",
    "            if len(block) >= 5:\n",
    "                x0, y0, x1, y1, text = block[:5]\n",
    "                rect = fitz.Rect(x0, y0, x1, y1)\n",
    "                if not any(fitz.Rect(*box).intersects(rect) for box in table_boxes) and text.strip():\n",
    "                    text_blocks.append((i, y0, text.strip()))\n",
    "                    text_positions.append(y0)  # Save y-position\n",
    "\n",
    "    text_blocks.sort(key=lambda x: (x[0], x[1]))\n",
    "    full_text = \"\\n\".join(text for _, _, text in text_blocks)\n",
    "    chunks = chunking_workflow(full_text)\n",
    "    chunk_positions = text_positions[:len(chunks)]\n",
    "    elements = create_elements_with_metadata(\n",
    "        chunks, [t for t in tables], input_pdf, text_positions=chunk_positions, table_positions=table_positions\n",
    "    )\n",
    "    return src, ocr_doc, elements\n",
    "\n",
    "\n",
    "def extract_text_and_tables_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    text = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    tables = []\n",
    "    for table in doc.tables:\n",
    "        table_data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
    "        tables.append(table_data)\n",
    "    return text, tables\n",
    "\n",
    "def extract_text_and_tables_from_pptx(path):\n",
    "    prs = Presentation(path)\n",
    "    text, tables = [], []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                text.append(shape.text.strip())\n",
    "            if hasattr(shape, \"has_table\") and shape.has_table:\n",
    "                table = shape.table\n",
    "                table_data = [[cell.text_frame.text.strip() if cell.text_frame else \"\" for cell in row.cells] for row in table.rows]\n",
    "                tables.append(table_data)\n",
    "    return text, tables\n",
    "\n",
    "\n",
    "def save_processed_output(src, ocr_doc, elements, ocr_pdf_path):\n",
    "    texts = [e for e in elements if e['type'] == 'text']\n",
    "    tables = [e for e in elements if e['type'] == 'table']\n",
    "\n",
    "    if texts:\n",
    "        with open(\"text_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"text_chunks\": texts}, f, indent=2, ensure_ascii=False)\n",
    "    if tables:\n",
    "        with open(\"tables_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"tables\": tables}, f, indent=2, ensure_ascii=False)\n",
    "    if ocr_doc and len(ocr_doc) > 0 and ocr_pdf_path:\n",
    "        ocr_doc.save(ocr_pdf_path)\n",
    "\n",
    "    if ocr_doc: ocr_doc.close()\n",
    "    if src: src.close()\n",
    "\n",
    "    return {\"text_chunks\": texts, \"tables\": tables}\n",
    "\n",
    "\n",
    "def smart_file_processing(input_file, ocr_output_pdf=\"ocr_output.pdf\"):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"❌ File not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    ext = os.path.splitext(input_file)[1].lower()\n",
    "    print(f\"📁 Processing file: {input_file}\")\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        src, ocr_doc, elements = process_pdf_pages(input_file)\n",
    "        save_processed_output(src, ocr_doc, elements, ocr_output_pdf)\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        texts, tables = extract_text_and_tables_from_docx(input_file)\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        chunks = chunking_workflow(full_text)\n",
    "        elements = create_elements_with_metadata(\n",
    "        chunks, tables, input_file, text_positions=list(range(len(chunks))), \n",
    "        table_positions=list(range(len(tables))))\n",
    "        save_processed_output(None, None, elements, None)\n",
    "\n",
    "    elif ext == \".pptx\":\n",
    "        texts, tables = extract_text_and_tables_from_pptx(input_file)\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        chunks = chunking_workflow(full_text)\n",
    "        elements = create_elements_with_metadata(\n",
    "        chunks, tables, input_file,text_positions=list(range(len(chunks))),\n",
    "        table_positions=list(range(len(tables))))\n",
    "        save_processed_output(None, None, elements, None)\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Unsupported file type: {ext}\")\n",
    "\n",
    "    print(\"\\n✅ Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b72ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Processing file: ..\\data\\Shell - Financial Statement-Page1-5 1.pdf\n",
      "   📄 Processing Page 1/5\n",
      "      🔎 Searchable: ❌ No, requires OCR\n",
      "   📄 Processing Page 2/5\n",
      "      🔎 Searchable: ❌ No, requires OCR\n",
      "   📄 Processing Page 3/5\n",
      "      🔎 Searchable: ❌ No, requires OCR\n",
      "   📄 Processing Page 4/5\n",
      "      🔎 Searchable: ❌ No, requires OCR\n",
      "   📄 Processing Page 5/5\n",
      "      🔎 Searchable: ❌ No, requires OCR\n",
      "\n",
      "✅ Done!\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"..\\data\\Shell - Financial Statement-Page1-5 1.pdf\" \n",
    "smart_file_processing(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b87680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 | Type: text | Tokens: 311\n",
      "\n",
      "Total Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_tokens_per_chunk(file_path=\"text_output.json\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    chunks = data.get(\"text_chunks\", [])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_type = chunk.get(\"type\", \"unknown\")\n",
    "        token_count = chunk.get(\"token_count\", 0)\n",
    "        print(f\"Chunk {idx+1} | Type: {chunk_type} | Tokens: {token_count}\")\n",
    "\n",
    "    print(f\"\\nTotal Chunks: {len(chunks)}\")\n",
    "\n",
    "print_tokens_per_chunk(\"text_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc759b",
   "metadata": {},
   "source": [
    "# **Milvus Vector DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7fb85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔌 Connecting to Milvus...\n",
      "✅ Connected to Milvus.\n",
      "🤖 Loading sentence transformer model...\n",
      "✅ Model loaded.\n",
      "🗃 Creating/Loading Milvus collections...\n",
      "📁 Text Collection already exists.\n",
      "📁 Table Collection already exists.\n",
      "✅ Collections 'textcollections' and 'tablecollections' are ready.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, list_collections\n",
    "from mistralai import Mistral\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# --- 1. SETUP ---\n",
    "\n",
    "# ⚙️ Connect to Milvus\n",
    "print(\"🔌 Connecting to Milvus...\")\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "print(\"✅ Connected to Milvus.\")\n",
    "\n",
    "# 🤖 Load Embedding Model\n",
    "print(\"🤖 Loading sentence transformer model...\")\n",
    "text_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# --- 2. SCHEMA & COLLECTION DEFINITION ---\n",
    "\n",
    "# 🏗 Define a robust schema that includes a field for the original content\n",
    "common_fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"page_no\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "]\n",
    "text_schema = CollectionSchema(fields=common_fields, description=\"Text document chunks\")\n",
    "table_schema = CollectionSchema(fields=common_fields, description=\"Table document chunks\")\n",
    "\n",
    "# 🗃 Create/Load Collections\n",
    "print(\"🗃 Creating/Loading Milvus collections...\")\n",
    "\n",
    "TEXT_COLLECTION_NAME = \"textcollections\"\n",
    "TABLE_COLLECTION_NAME = \"tablecollections\"\n",
    "\n",
    "\n",
    "if TEXT_COLLECTION_NAME not in list_collections():\n",
    "    text_col = Collection(TEXT_COLLECTION_NAME, schema=text_schema)\n",
    "    print(\"✅ Text Collection created.\")\n",
    "else:\n",
    "    text_col = Collection(name=TEXT_COLLECTION_NAME) # Correctly assign here\n",
    "    print(\"📁 Text Collection already exists.\")\n",
    "\n",
    "if TABLE_COLLECTION_NAME not in list_collections():\n",
    "    table_col = Collection(TABLE_COLLECTION_NAME, schema=table_schema)\n",
    "    print(\"✅ Table Collection created.\")\n",
    "else:\n",
    "    table_col = Collection(name=TABLE_COLLECTION_NAME) # And here\n",
    "    print(\"📁 Table Collection already exists.\")\n",
    "\n",
    "print(f\"✅ Collections '{text_col.name}' and '{table_col.name}' are ready.\")\n",
    "\n",
    "# --- 3. CORE FUNCTIONS ---\n",
    "\n",
    "def embed_and_insert(collection, data, encoder, data_type):\n",
    "    \"\"\"\n",
    "    Embeds content and inserts it into a Milvus collection.\n",
    "    Handles different JSON structures and serializes table content.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    if not isinstance(data, list):\n",
    "        print(f\"⚠️ Data for '{data_type}' is not a processable list. Skipping insertion.\")\n",
    "        return\n",
    "\n",
    "    embeddings, sources, pages, types, contents = [], [], [], [], []\n",
    "    print(f\"📦 Preparing '{data_type}' data for insertion...\")\n",
    "    for i, chunk in enumerate(tqdm(data, desc=f\"Embedding {data_type}s\")):\n",
    "        content_str = \"\"\n",
    "        metadata = {}\n",
    "        if isinstance(chunk, dict):\n",
    "            possible_keys = [\"content\", \"text\", \"page_content\"]\n",
    "            for key in possible_keys:\n",
    "                if chunk.get(key):\n",
    "                    content_str = chunk[key]\n",
    "                    break\n",
    "            metadata = chunk.get(\"metadata\", {})\n",
    "        elif isinstance(chunk, str):\n",
    "            content_str = chunk\n",
    "        \n",
    "        if isinstance(content_str, list): # Serialize table data\n",
    "            content_str = json.dumps(content_str)\n",
    "        if not content_str.strip():\n",
    "            continue\n",
    "            \n",
    "        source = metadata.get(\"source_document\", \"unknown\")\n",
    "        try:\n",
    "            page_no = int(metadata.get(\"chunk_id\", i))\n",
    "        except (ValueError, TypeError):\n",
    "            page_no = i\n",
    "\n",
    "        emb = encoder.encode(content_str)\n",
    "        \n",
    "        embeddings.append(emb.tolist())\n",
    "        sources.append(source)\n",
    "        pages.append(page_no)\n",
    "        types.append(data_type)\n",
    "        contents.append(content_str)\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"⚠️ No valid '{data_type}' data found to insert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🚀 Inserting {len(embeddings)} '{data_type}' vectors into '{collection.name}'...\")\n",
    "    collection.insert([embeddings, sources, pages, types, contents])\n",
    "    collection.flush()\n",
    "    print(f\"✅ Data successfully inserted and flushed into '{collection.name}'.\")\n",
    "\n",
    "# --- 4. Retrieve results ---\n",
    "\n",
    "def retrieve(query, top_k=5):\n",
    "    \"\"\"Searches both text and table collections, then combines and re-ranks the results.\"\"\"\n",
    "\n",
    "    query_vec = text_encoder.encode(query).tolist()\n",
    "    text_col.load()\n",
    "    table_col.load()\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    \n",
    "    text_results = text_col.search(\n",
    "        data=[query_vec], \n",
    "        anns_field=\"embedding\", \n",
    "        param=search_params, \n",
    "        limit=top_k,\n",
    "        output_fields=[\"source\", \"page_no\", \"type\", \"content\"]\n",
    "    )\n",
    "    table_results = table_col.search(\n",
    "        data=[query_vec], \n",
    "        anns_field=\"embedding\", \n",
    "        param=search_params, \n",
    "        limit=top_k,\n",
    "        output_fields=[\"source\", \"page_no\", \"type\", \"content\"]\n",
    "    )\n",
    "    \n",
    "    combined = {}\n",
    "    all_results = text_results[0] + table_results[0]\n",
    "    for res in all_results:\n",
    "        key = (res.entity.get(\"source\"), res.entity.get(\"page_no\"))\n",
    "        if key not in combined or combined[key].distance > res.distance:\n",
    "            combined[key] = res\n",
    "            \n",
    "    return sorted(list(combined.values()), key=lambda x: x.distance)\n",
    "\n",
    "# ---- 5. Generate answer ---\n",
    "\n",
    "def rag_answer(query):\n",
    "    \"\"\"Performs the full RAG pipeline: retrieve, prompt, and generate.\n",
    "    Returns a tuple of (answer, retrieved_chunks) where retrieved_chunks is a list of dicts\n",
    "    containing content, source, page_no, and similarity score.\"\"\"\n",
    "    \n",
    "    retrieved_hits = retrieve(query, top_k=5)\n",
    "\n",
    "    if not retrieved_hits:\n",
    "        return \"I could not find any relevant information in the documents to answer your question.\", []\n",
    "\n",
    "    # Prepare the context block for generation\n",
    "    context_block = \"\\n\\n---\\n\\n\".join([hit.entity.get(\"content\", \"\") for hit in retrieved_hits])\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an expert AI assistant. Use only the provided context below to answer the user's question. \"\n",
    "        \"Your answer must be based solely on this context. If the context does not contain the answer, \"\n",
    "        \"state that you cannot answer based on the provided information.\\n\\n\"\n",
    "        f\"--- CONTEXT ---\\n{context_block}\\n\\n--- END CONTEXT ---\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found. Please create a .env file with your key.\")\n",
    "        \n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    print(\"🤖 Generating answer with Mistral AI...\")\n",
    "    response = client.chat.complete(\n",
    "        model=\"mistral-small-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"✅ Answer generated.\")\n",
    "    \n",
    "    # Prepare the retrieved chunks information\n",
    "    retrieved_chunks = []\n",
    "    for hit in retrieved_hits:\n",
    "        chunk_info = {\n",
    "            \"content\": hit.entity.get(\"content\", \"\"),\n",
    "            \"source\": hit.entity.get(\"source\", \"unknown\"),\n",
    "            \"page_no\": hit.entity.get(\"page_no\", 0),\n",
    "            \"similarity_score\": 1 - hit.distance,  # Convert distance to similarity (higher is better)\n",
    "            \"type\": hit.entity.get(\"type\", \"unknown\")\n",
    "        }\n",
    "        retrieved_chunks.append(chunk_info)\n",
    "    \n",
    "    return response.choices[0].message.content.strip(), retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79bb055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 Loading JSON data...\n",
      "✅ JSON data loaded successfully.\n",
      "📦 Preparing 'text' data for insertion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|██████████| 30/30 [00:02<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Inserting 30 'text' vectors into 'textcollections'...\n",
      "✅ Data successfully inserted and flushed into 'textcollections'.\n",
      "📦 Preparing 'table' data for insertion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding tables: 100%|██████████| 9/9 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Inserting 9 'table' vectors into 'tablecollections'...\n",
      "✅ Data successfully inserted and flushed into 'tablecollections'.\n",
      "\n",
      "🏗 Creating indexes for collections (if they don't exist)...\n",
      "✅ Index already exists for 'textcollections'.\n",
      "✅ Index already exists for 'tablecollections'.\n",
      "\n",
      "==================================================\n",
      "📊 Collection Status:\n",
      "  - Entities in 'textcollections': 93\n",
      "  - Entities in 'tablecollections': 33\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"\\n📥 Loading JSON data...\")\n",
    "try:\n",
    "    with open(\"text_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = json.load(f)[\"text_chunks\"]\n",
    "    with open(\"tables_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        table_data = json.load(f)[\"tables\"]\n",
    "    print(\"✅ JSON data loaded successfully.\")\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"❌ Error loading data: {e}. Please ensure your JSON files exist and are correctly formatted.\")\n",
    "    exit()\n",
    "\n",
    "# --- Ingest Data ---\n",
    "embed_and_insert(collection=text_col, data=text_data, encoder=text_encoder, data_type=\"text\")\n",
    "embed_and_insert(collection=table_col, data=table_data, encoder=text_encoder, data_type=\"table\")\n",
    "\n",
    "# --- Create Indexes ---\n",
    "print(\"\\n🏗 Creating indexes for collections (if they don't exist)...\")\n",
    "index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "if not text_col.has_index():\n",
    "    text_col.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    print(f\"✅ Index created for '{text_col.name}'.\")\n",
    "else:\n",
    "    print(f\"✅ Index already exists for '{text_col.name}'.\")\n",
    "\n",
    "if not table_col.has_index():\n",
    "    table_col.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    print(f\"✅ Index created for '{table_col.name}'.\")\n",
    "else:\n",
    "    print(f\"✅ Index already exists for '{table_col.name}'.\")\n",
    "    \n",
    "# --- Verify & Query ---\n",
    "text_col.flush()\n",
    "table_col.flush()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 Collection Status:\")\n",
    "print(f\"  - Entities in '{text_col.name}': {text_col.num_entities}\")\n",
    "print(f\"  - Entities in '{table_col.name}': {table_col.num_entities}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21abc527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Generating answer with Mistral AI...\n",
      "✅ Answer generated.\n",
      "\n",
      "--------------------------------------------------\n",
      "❓ Query: What are the two main components of the Pixtral architecture?\n",
      "💡 Answer: The two main components of the Pixtral architecture are:\n",
      "1. A **vision encoder**, which tokenizes images.\n",
      "2. A **multimodal decoder**, which predicts the next text token given a sequence of text and images.\n",
      "\n",
      "🔍 Retrieved Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Source: mistral.pdf (Page 5)\n",
      "Type: text\n",
      "Similarity (out of 10): 0.04\n",
      "Content: relative position encodings lend themselves naturally to variable image sizes.\n",
      "3\n",
      "Figure 3: Complete Pixtral Architecture. Pixtral has two components: a vision encoder, which tokenizes\n",
      "images, and a mu...\n",
      "\n",
      "Chunk 2:\n",
      "Source: mistral.docx (Page 12)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.23\n",
      "Content:  performance across various benchmarks, outperforming other open models and matching larger models. Its superior instruction following abilities, support for variable image sizes, and long context win...\n",
      "\n",
      "Chunk 3:\n",
      "Source: mistral.pdf (Page 16)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.38\n",
      "Content: Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh,\n",
      "Amélie Héliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume\n",
      "Lample, Diego La...\n",
      "\n",
      "Chunk 4:\n",
      "Source: mistral.pdf (Page 13)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.83\n",
      "Content: -of-the-art multimodal model that excels in both text-onlyand multimodal tasks. With a novel architecture featuring a 400M-parameter vision encoder anda 12B-parameter multimodal decoder, Pixtral 12B d...\n",
      "\n",
      "Chunk 5:\n",
      "Source: mistral.pdf (Page 1)\n",
      "Type: text\n",
      "Similarity (out of 10): -1.14\n",
      "Content: Pixtral 12BarXiv:2410.07073v2  [cs.CV]  10 Oct 2024AbstractWe introduce Pixtral 12B, a 12–billion-parameter multimodal language model.Pixtral 12B is trained to understand both natural images and docum...\n",
      "\n",
      "Chunk 6:\n",
      "Source: mistral.docx (Page 34)\n",
      "Type: table\n",
      "Similarity (out of 10): -6.88\n",
      "Content: [[\"\", \"\", \"\", \"\"], [\"\", \"Mathvista\\n\\nFlexible Level 3\", \"MMMU\\n\\nFlexible Level 3\", \"ChartQA\\n\\nFlexible Level 3\"], [\"Llama-3.2 11B [6]\", \"42.1 (\\u00b11.9)\", \"45.3 (\\u00b11.0)\", \"77.2 (\\u00b10.8)\"], ...\n",
      "\n",
      "Chunk 7:\n",
      "Source: mistral.docx (Page 28)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.34\n",
      "Content: [[\"\", \"Mathvista\", \"MMMU\", \"ChartQA\", \"DocVQA\", \"VQAv2\", \"MM-MT-Bench\", \"LMSys-Vision\"], [\"\", \"CoT\", \"CoT\", \"CoT\", \"ANLS\", \"VQA Match\", \"GPT-4o Judge\", \"(Oct \\u201924)\"], [\"Pixtral 12B\", \"58.3\", \"52.0...\n",
      "\n",
      "Chunk 8:\n",
      "Source: mistral.docx (Page 35)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.43\n",
      "Content: [[\"\", \"Mathvista\", \"MMMU\", \"ChartQA\", \"DocVQA\", \"VQAv2\", \"MM-MT-Bench\", \"LMSys-Vision\"], [\"\", \"CoT\", \"CoT\", \"CoT\", \"ANLS\", \"VQA Match\", \"GPT-4o Judge\", \"(Oct \\u201924)\"], [\"Pixtral 12B\", \"58.3\", \"52.0...\n",
      "\n",
      "Chunk 9:\n",
      "Source: mistral.docx (Page 31)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.65\n",
      "Content: [[\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"], [\"\", \"VQAv2\", \"VQAv2\", \"ChartQA\", \"ChartQA\", \"MMMU\", \"MMMU\", \"\"], [\"Prompt \\u2212\\u2192\", \"Naive\", \"Explicit\", \"Naive\", \"Explicit\", \"Naive\", \"Explicit\", \"\"], [\"GPT-4...\n",
      "\n",
      "Chunk 10:\n",
      "Source: Shell - Financial Statement-Page1-5 1.pdf (Page 2)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.72\n",
      "Content: [{\"238\": \"238\", \"Consolidated Statement of Income\": \"Consolidated Statement of Comprehensive Income\"}, {\"238\": \"239\", \"Consolidated Statement of Income\": \"Consolidated Balance Sheet\"}, {\"238\": \"240\", ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the two main components of the Pixtral architecture?\"\n",
    "\n",
    "answer, chunks = rag_answer(query)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"❓ Query: {query}\")\n",
    "print(f\"💡 Answer: {answer}\")\n",
    "print(\"\\n🔍 Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Source: {chunk['source']} (Page {chunk['page_no']})\")\n",
    "    print(f\"Type: {chunk['type']}\")\n",
    "    print(f\"Similarity (out of 10): {chunk['similarity_score'] * 10:.2f}\")\n",
    "    print(f\"Content: {chunk['content'][:200]}...\") \n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41367a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
