{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02789cb",
   "metadata": {},
   "source": [
    "# **Install Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a7db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF -q\n",
    "%pip install gmft -q\n",
    "%pip install sentence-transformers mistralai -q\n",
    "%pip install mistralai --upgrade -q\n",
    "%pip install langchain -q\n",
    "%pip install langchain-community -q\n",
    "%pip install langchain_experimental -q\n",
    "%pip install -U langchain-huggingface -q\n",
    "%pip install --upgrade langchain -q\n",
    "%pip install tiktoken -q\n",
    "%pip install ftfy -q\n",
    "%pip install python-pptx python-docx -q\n",
    "%pip install --upgrade pymilvus requests tqdm tf-keras -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e20ae",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac431de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\rag_app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import ftfy\n",
    "import tiktoken\n",
    "import collections\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from gmft.auto import AutoTableDetector, AutoTableFormatter\n",
    "from gmft.pdf_bindings import PyPDFium2Document\n",
    "\n",
    "# Configration \n",
    "detector = AutoTableDetector()\n",
    "formatter = AutoTableFormatter()\n",
    "\n",
    "tesseract_path = os.path.join(os.getcwd(), \".venv\", \"Tesseract-OCR\", \"tesseract.exe\")\n",
    "os.environ[\"TESSERACT_PATH\"] = tesseract_path\n",
    "\n",
    "DPI = 150  # For OCR resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698b906",
   "metadata": {},
   "source": [
    "# **Document Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efecce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_page_searchable(page, min_chars=10):\n",
    "    return len(page.get_text().strip()) >= min_chars\n",
    "\n",
    "def process_ocr(page, dpi=150, language=None):\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    ocr_pdf_bytes = pix.pdfocr_tobytes(language=language)\n",
    "    return fitz.open(\"pdf\", ocr_pdf_bytes)\n",
    "\n",
    "def extract_tables(pdf_source, page_number):\n",
    "    doc = PyPDFium2Document(pdf_source)\n",
    "    tables = []\n",
    "    try:\n",
    "        page = doc[page_number]\n",
    "        for cropped in detector.extract(page):\n",
    "            formatted = formatter.extract(cropped, margin=\"auto\", padding=None)\n",
    "            df = formatted.df()\n",
    "            tables.append((cropped.bbox, df))\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return tables\n",
    "\n",
    "def extract_tables_from_doc(fitz_doc, page_number):\n",
    "    single_page_doc = fitz.open()\n",
    "    single_page_doc.insert_pdf(fitz_doc, from_page=page_number, to_page=page_number)\n",
    "    pdf_bytes = single_page_doc.tobytes()\n",
    "    single_page_doc.close()\n",
    "    return extract_tables(pdf_bytes, 0)\n",
    "\n",
    "def uniquify_columns(cols):\n",
    "    counts = collections.Counter()\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        counts[col] += 1\n",
    "        new_cols.append(col if counts[col] == 1 else f\"{col}_{counts[col] - 1}\")\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd505",
   "metadata": {},
   "source": [
    "# **Chunking and Text Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32059d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_utf8(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.replace(\"\\x00\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text = text.replace('\\n', '').replace('\\\"', '')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def custom_chunking(text, chunk_size=500, overlap=50):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    chunks, start = [], 0\n",
    "    while start < len(token_ids):\n",
    "        end = min(start + chunk_size, len(token_ids))\n",
    "        chunk_token_ids = token_ids[start:end]\n",
    "        chunks.append(tokenizer.decode(chunk_token_ids))\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def chunking_workflow(text, max_tokens=500, overlap=50):\n",
    "    clean_text = process_text_utf8(text)\n",
    "    if not clean_text.strip():\n",
    "        return []\n",
    "    token_count = count_tokens(clean_text)\n",
    "    if token_count <= max_tokens:\n",
    "        return [{\"text\": clean_text, \"metadata\": {}, \"token_count\": token_count}]\n",
    "    chunks = custom_chunking(clean_text, chunk_size=max_tokens, overlap=overlap)\n",
    "    return [\n",
    "        {\"text\": chunk, \"metadata\": {}, \"token_count\": count_tokens(chunk)}\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "def create_elements_with_metadata(chunks, tables, input_file, text_positions=None, table_positions=None):\n",
    "    elements = []\n",
    "    base_name = os.path.basename(input_file)\n",
    "    # Add text chunks with position if available\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "        metadata.update({\n",
    "            \"source_document\": base_name, \"chunk_id\": i, \n",
    "            \"position\": text_positions[i-1] if text_positions and i-1 < len(text_positions) else None})\n",
    "        elements.append({\"type\": \"text\",\"content\": chunk['text'],\"metadata\": metadata,\"token_count\": chunk['token_count']})\n",
    "\n",
    "    # Add tables with position if available\n",
    "    for j, table in enumerate(tables, start=i+1):\n",
    "        if isinstance(table, dict) and \"metadata\" in table:\n",
    "            # PDF table with metadata\n",
    "            meta = table[\"metadata\"].copy()\n",
    "            meta.update({\"source_document\": base_name,\"chunk_id\": j})\n",
    "            content = table.get(\"content\", table)\n",
    "        else:\n",
    "            meta = {\n",
    "                \"source_document\": base_name,\"chunk_id\": j, \n",
    "                \"position\": table_positions[j - (i+1)] if table_positions and (j - (i+1)) < len(table_positions) else j}\n",
    "            content = table\n",
    "        elements.append({\"type\": \"table\",\"content\": content,\"metadata\": meta})\n",
    "\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb577743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_pages(input_pdf):\n",
    "    src = fitz.open(input_pdf)\n",
    "    ocr_doc = fitz.open()\n",
    "    tables, text_blocks = [], []\n",
    "    text_positions, table_positions = [], []\n",
    "    filename = os.path.basename(input_pdf)\n",
    "\n",
    "    for i, page in enumerate(src):\n",
    "        print(f\"   ðŸ“„ Processing Page {i + 1}/{len(src)}\")\n",
    "        searchable = is_page_searchable(page)\n",
    "        temp_doc = None  # To manage OCR temp doc lifetime\n",
    "        if not searchable:\n",
    "            print(f\"      ðŸ”Ž Searchable: âŒ No, requires OCR\")\n",
    "            temp_doc = process_ocr(page, dpi=DPI)\n",
    "            blocks_page = temp_doc[0]\n",
    "            page_tables = extract_tables_from_doc(temp_doc, 0)\n",
    "            ocr_doc.insert_pdf(temp_doc)\n",
    "        else:\n",
    "            print(f\"      ðŸ”Ž Searchable: âœ… Yes\")\n",
    "            blocks_page = page\n",
    "            page_tables = extract_tables_from_doc(src, i)\n",
    "\n",
    "        for idx, (bbox, df) in enumerate(page_tables, 1):\n",
    "            df.columns = uniquify_columns(df.columns.astype(str))\n",
    "            tables.append({\n",
    "                \"type\": \"table\",\n",
    "                \"content\": df.to_dict(orient=\"records\"),\n",
    "                \"metadata\": {\"page_number\": i + 1,\"columns\": df.columns.tolist(),\n",
    "                    \"table_index_on_page\": idx,\"position\": bbox[1], \"source_document\": filename}})\n",
    "            table_positions.append(bbox[1])  # Save y-position\n",
    "\n",
    "        table_boxes = [box for box, _ in page_tables]\n",
    "        for block in blocks_page.get_text(\"blocks\"):\n",
    "            if len(block) >= 5:\n",
    "                x0, y0, x1, y1, text = block[:5]\n",
    "                rect = fitz.Rect(x0, y0, x1, y1)\n",
    "                if not any(fitz.Rect(*box).intersects(rect) for box in table_boxes) and text.strip():\n",
    "                    text_blocks.append((i, y0, text.strip()))\n",
    "                    text_positions.append(y0)  # Save y-position\n",
    "\n",
    "    text_blocks.sort(key=lambda x: (x[0], x[1]))\n",
    "    full_text = \"\\n\".join(text for _, _, text in text_blocks)\n",
    "    chunks = chunking_workflow(full_text)\n",
    "    chunk_positions = text_positions[:len(chunks)]\n",
    "    elements = create_elements_with_metadata(\n",
    "        chunks, [t for t in tables], input_pdf, text_positions=chunk_positions, table_positions=table_positions\n",
    "    )\n",
    "    return src, ocr_doc, elements\n",
    "\n",
    "\n",
    "def extract_text_and_tables_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    text = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "    tables = []\n",
    "    for table in doc.tables:\n",
    "        table_data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
    "        tables.append(table_data)\n",
    "    return text, tables\n",
    "\n",
    "def extract_text_and_tables_from_pptx(path):\n",
    "    prs = Presentation(path)\n",
    "    text, tables = [], []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                text.append(shape.text.strip())\n",
    "            if hasattr(shape, \"has_table\") and shape.has_table:\n",
    "                table = shape.table\n",
    "                table_data = [[cell.text_frame.text.strip() if cell.text_frame else \"\" for cell in row.cells] for row in table.rows]\n",
    "                tables.append(table_data)\n",
    "    return text, tables\n",
    "\n",
    "\n",
    "def save_processed_output(src, ocr_doc, elements, ocr_pdf_path):\n",
    "    texts = [e for e in elements if e['type'] == 'text']\n",
    "    tables = [e for e in elements if e['type'] == 'table']\n",
    "\n",
    "    if texts:\n",
    "        with open(\"text_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"text_chunks\": texts}, f, indent=2, ensure_ascii=False)\n",
    "    if tables:\n",
    "        with open(\"tables_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"tables\": tables}, f, indent=2, ensure_ascii=False)\n",
    "    if ocr_doc and len(ocr_doc) > 0 and ocr_pdf_path:\n",
    "        ocr_doc.save(ocr_pdf_path)\n",
    "\n",
    "    if ocr_doc: ocr_doc.close()\n",
    "    if src: src.close()\n",
    "\n",
    "    return {\"text_chunks\": texts, \"tables\": tables}\n",
    "\n",
    "\n",
    "def smart_file_processing(input_file, ocr_output_pdf=\"ocr_output.pdf\"):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"âŒ File not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    ext = os.path.splitext(input_file)[1].lower()\n",
    "    print(f\"ðŸ“ Processing file: {input_file}\")\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        src, ocr_doc, elements = process_pdf_pages(input_file)\n",
    "        save_processed_output(src, ocr_doc, elements, ocr_output_pdf)\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        texts, tables = extract_text_and_tables_from_docx(input_file)\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        chunks = chunking_workflow(full_text)\n",
    "        elements = create_elements_with_metadata(\n",
    "        chunks, tables, input_file, text_positions=list(range(len(chunks))), \n",
    "        table_positions=list(range(len(tables))))\n",
    "        save_processed_output(None, None, elements, None)\n",
    "\n",
    "    elif ext == \".pptx\":\n",
    "        texts, tables = extract_text_and_tables_from_pptx(input_file)\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        chunks = chunking_workflow(full_text)\n",
    "        elements = create_elements_with_metadata(\n",
    "        chunks, tables, input_file,text_positions=list(range(len(chunks))),\n",
    "        table_positions=list(range(len(tables))))\n",
    "        save_processed_output(None, None, elements, None)\n",
    "\n",
    "    else:\n",
    "        print(f\"âŒ Unsupported file type: {ext}\")\n",
    "\n",
    "    print(\"\\nâœ… Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b72ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Processing file: ..\\data\\Shell - Financial Statement-Page1-5 1.pdf\n",
      "   ðŸ“„ Processing Page 1/5\n",
      "      ðŸ”Ž Searchable: âŒ No, requires OCR\n",
      "   ðŸ“„ Processing Page 2/5\n",
      "      ðŸ”Ž Searchable: âŒ No, requires OCR\n",
      "   ðŸ“„ Processing Page 3/5\n",
      "      ðŸ”Ž Searchable: âŒ No, requires OCR\n",
      "   ðŸ“„ Processing Page 4/5\n",
      "      ðŸ”Ž Searchable: âŒ No, requires OCR\n",
      "   ðŸ“„ Processing Page 5/5\n",
      "      ðŸ”Ž Searchable: âŒ No, requires OCR\n",
      "\n",
      "âœ… Done!\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"..\\data\\Shell - Financial Statement-Page1-5 1.pdf\" \n",
    "smart_file_processing(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b87680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 | Type: text | Tokens: 311\n",
      "\n",
      "Total Chunks: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_tokens_per_chunk(file_path=\"text_output.json\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    chunks = data.get(\"text_chunks\", [])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_type = chunk.get(\"type\", \"unknown\")\n",
    "        token_count = chunk.get(\"token_count\", 0)\n",
    "        print(f\"Chunk {idx+1} | Type: {chunk_type} | Tokens: {token_count}\")\n",
    "\n",
    "    print(f\"\\nTotal Chunks: {len(chunks)}\")\n",
    "\n",
    "print_tokens_per_chunk(\"text_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc759b",
   "metadata": {},
   "source": [
    "# **Milvus Vector DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7fb85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Œ Connecting to Milvus...\n",
      "âœ… Connected to Milvus.\n",
      "ðŸ¤– Loading sentence transformer model...\n",
      "âœ… Model loaded.\n",
      "ðŸ—ƒ Creating/Loading Milvus collections...\n",
      "ðŸ“ Text Collection already exists.\n",
      "ðŸ“ Table Collection already exists.\n",
      "âœ… Collections 'textcollections' and 'tablecollections' are ready.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, list_collections\n",
    "from mistralai import Mistral\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# --- 1. SETUP ---\n",
    "\n",
    "# âš™ï¸ Connect to Milvus\n",
    "print(\"ðŸ”Œ Connecting to Milvus...\")\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "print(\"âœ… Connected to Milvus.\")\n",
    "\n",
    "# ðŸ¤– Load Embedding Model\n",
    "print(\"ðŸ¤– Loading sentence transformer model...\")\n",
    "text_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"âœ… Model loaded.\")\n",
    "\n",
    "# --- 2. SCHEMA & COLLECTION DEFINITION ---\n",
    "\n",
    "# ðŸ— Define a robust schema that includes a field for the original content\n",
    "common_fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=512),\n",
    "    FieldSchema(name=\"page_no\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "]\n",
    "text_schema = CollectionSchema(fields=common_fields, description=\"Text document chunks\")\n",
    "table_schema = CollectionSchema(fields=common_fields, description=\"Table document chunks\")\n",
    "\n",
    "# ðŸ—ƒ Create/Load Collections\n",
    "print(\"ðŸ—ƒ Creating/Loading Milvus collections...\")\n",
    "\n",
    "TEXT_COLLECTION_NAME = \"textcollections\"\n",
    "TABLE_COLLECTION_NAME = \"tablecollections\"\n",
    "\n",
    "\n",
    "if TEXT_COLLECTION_NAME not in list_collections():\n",
    "    text_col = Collection(TEXT_COLLECTION_NAME, schema=text_schema)\n",
    "    print(\"âœ… Text Collection created.\")\n",
    "else:\n",
    "    text_col = Collection(name=TEXT_COLLECTION_NAME) # Correctly assign here\n",
    "    print(\"ðŸ“ Text Collection already exists.\")\n",
    "\n",
    "if TABLE_COLLECTION_NAME not in list_collections():\n",
    "    table_col = Collection(TABLE_COLLECTION_NAME, schema=table_schema)\n",
    "    print(\"âœ… Table Collection created.\")\n",
    "else:\n",
    "    table_col = Collection(name=TABLE_COLLECTION_NAME) # And here\n",
    "    print(\"ðŸ“ Table Collection already exists.\")\n",
    "\n",
    "print(f\"âœ… Collections '{text_col.name}' and '{table_col.name}' are ready.\")\n",
    "\n",
    "# --- 3. CORE FUNCTIONS ---\n",
    "\n",
    "def embed_and_insert(collection, data, encoder, data_type):\n",
    "    \"\"\"\n",
    "    Embeds content and inserts it into a Milvus collection.\n",
    "    Handles different JSON structures and serializes table content.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    if not isinstance(data, list):\n",
    "        print(f\"âš ï¸ Data for '{data_type}' is not a processable list. Skipping insertion.\")\n",
    "        return\n",
    "\n",
    "    embeddings, sources, pages, types, contents = [], [], [], [], []\n",
    "    print(f\"ðŸ“¦ Preparing '{data_type}' data for insertion...\")\n",
    "    for i, chunk in enumerate(tqdm(data, desc=f\"Embedding {data_type}s\")):\n",
    "        content_str = \"\"\n",
    "        metadata = {}\n",
    "        if isinstance(chunk, dict):\n",
    "            possible_keys = [\"content\", \"text\", \"page_content\"]\n",
    "            for key in possible_keys:\n",
    "                if chunk.get(key):\n",
    "                    content_str = chunk[key]\n",
    "                    break\n",
    "            metadata = chunk.get(\"metadata\", {})\n",
    "        elif isinstance(chunk, str):\n",
    "            content_str = chunk\n",
    "        \n",
    "        if isinstance(content_str, list): # Serialize table data\n",
    "            content_str = json.dumps(content_str)\n",
    "        if not content_str.strip():\n",
    "            continue\n",
    "            \n",
    "        source = metadata.get(\"source_document\", \"unknown\")\n",
    "        try:\n",
    "            page_no = int(metadata.get(\"chunk_id\", i))\n",
    "        except (ValueError, TypeError):\n",
    "            page_no = i\n",
    "\n",
    "        emb = encoder.encode(content_str)\n",
    "        \n",
    "        embeddings.append(emb.tolist())\n",
    "        sources.append(source)\n",
    "        pages.append(page_no)\n",
    "        types.append(data_type)\n",
    "        contents.append(content_str)\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"âš ï¸ No valid '{data_type}' data found to insert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸš€ Inserting {len(embeddings)} '{data_type}' vectors into '{collection.name}'...\")\n",
    "    collection.insert([embeddings, sources, pages, types, contents])\n",
    "    collection.flush()\n",
    "    print(f\"âœ… Data successfully inserted and flushed into '{collection.name}'.\")\n",
    "\n",
    "# --- 4. Retrieve results ---\n",
    "\n",
    "def retrieve(query, top_k=5):\n",
    "    \"\"\"Searches both text and table collections, then combines and re-ranks the results.\"\"\"\n",
    "\n",
    "    query_vec = text_encoder.encode(query).tolist()\n",
    "    text_col.load()\n",
    "    table_col.load()\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    \n",
    "    text_results = text_col.search(\n",
    "        data=[query_vec], \n",
    "        anns_field=\"embedding\", \n",
    "        param=search_params, \n",
    "        limit=top_k,\n",
    "        output_fields=[\"source\", \"page_no\", \"type\", \"content\"]\n",
    "    )\n",
    "    table_results = table_col.search(\n",
    "        data=[query_vec], \n",
    "        anns_field=\"embedding\", \n",
    "        param=search_params, \n",
    "        limit=top_k,\n",
    "        output_fields=[\"source\", \"page_no\", \"type\", \"content\"]\n",
    "    )\n",
    "    \n",
    "    combined = {}\n",
    "    all_results = text_results[0] + table_results[0]\n",
    "    for res in all_results:\n",
    "        key = (res.entity.get(\"source\"), res.entity.get(\"page_no\"))\n",
    "        if key not in combined or combined[key].distance > res.distance:\n",
    "            combined[key] = res\n",
    "            \n",
    "    return sorted(list(combined.values()), key=lambda x: x.distance)\n",
    "\n",
    "# ---- 5. Generate answer ---\n",
    "\n",
    "def rag_answer(query):\n",
    "    \"\"\"Performs the full RAG pipeline: retrieve, prompt, and generate.\n",
    "    Returns a tuple of (answer, retrieved_chunks) where retrieved_chunks is a list of dicts\n",
    "    containing content, source, page_no, and similarity score.\"\"\"\n",
    "    \n",
    "    retrieved_hits = retrieve(query, top_k=5)\n",
    "\n",
    "    if not retrieved_hits:\n",
    "        return \"I could not find any relevant information in the documents to answer your question.\", []\n",
    "\n",
    "    # Prepare the context block for generation\n",
    "    context_block = \"\\n\\n---\\n\\n\".join([hit.entity.get(\"content\", \"\") for hit in retrieved_hits])\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an expert AI assistant. Use only the provided context below to answer the user's question. \"\n",
    "        \"Your answer must be based solely on this context. If the context does not contain the answer, \"\n",
    "        \"state that you cannot answer based on the provided information.\\n\\n\"\n",
    "        f\"--- CONTEXT ---\\n{context_block}\\n\\n--- END CONTEXT ---\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"MISTRAL_API_KEY not found. Please create a .env file with your key.\")\n",
    "        \n",
    "    client = Mistral(api_key=api_key)\n",
    "\n",
    "    print(\"ðŸ¤– Generating answer with Mistral AI...\")\n",
    "    response = client.chat.complete(\n",
    "        model=\"mistral-small-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"âœ… Answer generated.\")\n",
    "    \n",
    "    # Prepare the retrieved chunks information\n",
    "    retrieved_chunks = []\n",
    "    for hit in retrieved_hits:\n",
    "        chunk_info = {\n",
    "            \"content\": hit.entity.get(\"content\", \"\"),\n",
    "            \"source\": hit.entity.get(\"source\", \"unknown\"),\n",
    "            \"page_no\": hit.entity.get(\"page_no\", 0),\n",
    "            \"similarity_score\": 1 - hit.distance,  # Convert distance to similarity (higher is better)\n",
    "            \"type\": hit.entity.get(\"type\", \"unknown\")\n",
    "        }\n",
    "        retrieved_chunks.append(chunk_info)\n",
    "    \n",
    "    return response.choices[0].message.content.strip(), retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79bb055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Loading JSON data...\n",
      "âœ… JSON data loaded successfully.\n",
      "ðŸ“¦ Preparing 'text' data for insertion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:02<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Inserting 30 'text' vectors into 'textcollections'...\n",
      "âœ… Data successfully inserted and flushed into 'textcollections'.\n",
      "ðŸ“¦ Preparing 'table' data for insertion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding tables: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Inserting 9 'table' vectors into 'tablecollections'...\n",
      "âœ… Data successfully inserted and flushed into 'tablecollections'.\n",
      "\n",
      "ðŸ— Creating indexes for collections (if they don't exist)...\n",
      "âœ… Index already exists for 'textcollections'.\n",
      "âœ… Index already exists for 'tablecollections'.\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š Collection Status:\n",
      "  - Entities in 'textcollections': 93\n",
      "  - Entities in 'tablecollections': 33\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"\\nðŸ“¥ Loading JSON data...\")\n",
    "try:\n",
    "    with open(\"text_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = json.load(f)[\"text_chunks\"]\n",
    "    with open(\"tables_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        table_data = json.load(f)[\"tables\"]\n",
    "    print(\"âœ… JSON data loaded successfully.\")\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"âŒ Error loading data: {e}. Please ensure your JSON files exist and are correctly formatted.\")\n",
    "    exit()\n",
    "\n",
    "# --- Ingest Data ---\n",
    "embed_and_insert(collection=text_col, data=text_data, encoder=text_encoder, data_type=\"text\")\n",
    "embed_and_insert(collection=table_col, data=table_data, encoder=text_encoder, data_type=\"table\")\n",
    "\n",
    "# --- Create Indexes ---\n",
    "print(\"\\nðŸ— Creating indexes for collections (if they don't exist)...\")\n",
    "index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "if not text_col.has_index():\n",
    "    text_col.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    print(f\"âœ… Index created for '{text_col.name}'.\")\n",
    "else:\n",
    "    print(f\"âœ… Index already exists for '{text_col.name}'.\")\n",
    "\n",
    "if not table_col.has_index():\n",
    "    table_col.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    print(f\"âœ… Index created for '{table_col.name}'.\")\n",
    "else:\n",
    "    print(f\"âœ… Index already exists for '{table_col.name}'.\")\n",
    "    \n",
    "# --- Verify & Query ---\n",
    "text_col.flush()\n",
    "table_col.flush()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š Collection Status:\")\n",
    "print(f\"  - Entities in '{text_col.name}': {text_col.num_entities}\")\n",
    "print(f\"  - Entities in '{table_col.name}': {table_col.num_entities}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21abc527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Generating answer with Mistral AI...\n",
      "âœ… Answer generated.\n",
      "\n",
      "--------------------------------------------------\n",
      "â“ Query: What are the two main components of the Pixtral architecture?\n",
      "ðŸ’¡ Answer: The two main components of the Pixtral architecture are:\n",
      "1. A **vision encoder**, which tokenizes images.\n",
      "2. A **multimodal decoder**, which predicts the next text token given a sequence of text and images.\n",
      "\n",
      "ðŸ” Retrieved Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Source: mistral.pdf (Page 5)\n",
      "Type: text\n",
      "Similarity (out of 10): 0.04\n",
      "Content: relative position encodings lend themselves naturally to variable image sizes.\n",
      "3\n",
      "Figure 3: Complete Pixtral Architecture. Pixtral has two components: a vision encoder, which tokenizes\n",
      "images, and a mu...\n",
      "\n",
      "Chunk 2:\n",
      "Source: mistral.docx (Page 12)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.23\n",
      "Content:  performance across various benchmarks, outperforming other open models and matching larger models. Its superior instruction following abilities, support for variable image sizes, and long context win...\n",
      "\n",
      "Chunk 3:\n",
      "Source: mistral.pdf (Page 16)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.38\n",
      "Content: Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh,\n",
      "AmÃ©lie HÃ©liou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, TimothÃ©e Lacroix, Guillaume\n",
      "Lample, Diego La...\n",
      "\n",
      "Chunk 4:\n",
      "Source: mistral.pdf (Page 13)\n",
      "Type: text\n",
      "Similarity (out of 10): -0.83\n",
      "Content: -of-the-art multimodal model that excels in both text-onlyand multimodal tasks. With a novel architecture featuring a 400M-parameter vision encoder anda 12B-parameter multimodal decoder, Pixtral 12B d...\n",
      "\n",
      "Chunk 5:\n",
      "Source: mistral.pdf (Page 1)\n",
      "Type: text\n",
      "Similarity (out of 10): -1.14\n",
      "Content: Pixtral 12BarXiv:2410.07073v2  [cs.CV]  10 Oct 2024AbstractWe introduce Pixtral 12B, a 12â€“billion-parameter multimodal language model.Pixtral 12B is trained to understand both natural images and docum...\n",
      "\n",
      "Chunk 6:\n",
      "Source: mistral.docx (Page 34)\n",
      "Type: table\n",
      "Similarity (out of 10): -6.88\n",
      "Content: [[\"\", \"\", \"\", \"\"], [\"\", \"Mathvista\\n\\nFlexible Level 3\", \"MMMU\\n\\nFlexible Level 3\", \"ChartQA\\n\\nFlexible Level 3\"], [\"Llama-3.2 11B [6]\", \"42.1 (\\u00b11.9)\", \"45.3 (\\u00b11.0)\", \"77.2 (\\u00b10.8)\"], ...\n",
      "\n",
      "Chunk 7:\n",
      "Source: mistral.docx (Page 28)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.34\n",
      "Content: [[\"\", \"Mathvista\", \"MMMU\", \"ChartQA\", \"DocVQA\", \"VQAv2\", \"MM-MT-Bench\", \"LMSys-Vision\"], [\"\", \"CoT\", \"CoT\", \"CoT\", \"ANLS\", \"VQA Match\", \"GPT-4o Judge\", \"(Oct \\u201924)\"], [\"Pixtral 12B\", \"58.3\", \"52.0...\n",
      "\n",
      "Chunk 8:\n",
      "Source: mistral.docx (Page 35)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.43\n",
      "Content: [[\"\", \"Mathvista\", \"MMMU\", \"ChartQA\", \"DocVQA\", \"VQAv2\", \"MM-MT-Bench\", \"LMSys-Vision\"], [\"\", \"CoT\", \"CoT\", \"CoT\", \"ANLS\", \"VQA Match\", \"GPT-4o Judge\", \"(Oct \\u201924)\"], [\"Pixtral 12B\", \"58.3\", \"52.0...\n",
      "\n",
      "Chunk 9:\n",
      "Source: mistral.docx (Page 31)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.65\n",
      "Content: [[\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"], [\"\", \"VQAv2\", \"VQAv2\", \"ChartQA\", \"ChartQA\", \"MMMU\", \"MMMU\", \"\"], [\"Prompt \\u2212\\u2192\", \"Naive\", \"Explicit\", \"Naive\", \"Explicit\", \"Naive\", \"Explicit\", \"\"], [\"GPT-4...\n",
      "\n",
      "Chunk 10:\n",
      "Source: Shell - Financial Statement-Page1-5 1.pdf (Page 2)\n",
      "Type: table\n",
      "Similarity (out of 10): -7.72\n",
      "Content: [{\"238\": \"238\", \"Consolidated Statement of Income\": \"Consolidated Statement of Comprehensive Income\"}, {\"238\": \"239\", \"Consolidated Statement of Income\": \"Consolidated Balance Sheet\"}, {\"238\": \"240\", ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the two main components of the Pixtral architecture?\"\n",
    "\n",
    "answer, chunks = rag_answer(query)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"â“ Query: {query}\")\n",
    "print(f\"ðŸ’¡ Answer: {answer}\")\n",
    "print(\"\\nðŸ” Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Source: {chunk['source']} (Page {chunk['page_no']})\")\n",
    "    print(f\"Type: {chunk['type']}\")\n",
    "    print(f\"Similarity (out of 10): {chunk['similarity_score'] * 10:.2f}\")\n",
    "    print(f\"Content: {chunk['content'][:200]}...\") \n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41367a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
